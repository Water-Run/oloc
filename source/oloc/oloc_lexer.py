r"""
:author: WaterRun
:date: 2025-03-21
:file: oloc_lexer.py
:description: Oloc lexer
"""

import re
import time
from enum import Enum
import oloc_utils as utils
from oloc_evaluator import Evaluator
from oloc_exceptions import *
from oloc_token import Token


class Lexer:
    r"""
    è¯æ³•åˆ†æå™¨
    :param expression: å¾…å¤„ç†çš„è¡¨è¾¾å¼
    """

    def __init__(self, expression: str):
        self.expression = expression
        self.tokens: list[Token] = []
        self.time_cost = -1

    def _convert_token_flow(self) -> None:
        r"""
        å°†è¡¨è¾¾å¼è½¬ä¸ºTokenæµ,å¹¶æ£€æŸ¥Tokençš„åˆæ³•æ€§
        :return: None
        :raise OlocInvalidTokenException: å¦‚æœTokenä¸åˆæ³•
        """
        self.tokens = Lexer.tokenizer(self.expression)
        for check_token in self.tokens:
            if not check_token.is_legal:
                raise OlocInvalidTokenException(
                    exception_type=check_token.get_exception_type(),
                    expression=self.expression,
                    positions=list(range(*[check_token.range[0], check_token.range[1]])),
                    token_content=check_token.value if check_token else "",
                )
            if check_token.type == Token.TYPE.LONG_CUSTOM and check_token.value.startswith('<__reserved'):
                raise OlocReservedWordException(
                    exception_type=OlocReservedWordException.EXCEPTION_TYPE.IS_RESERVED,
                    expression=self.expression,
                    positions=list(range(*[check_token.range[0], check_token.range[1]])),
                    conflict_str=check_token.value,
                )

    def _formal_complementation(self) -> None:
        r"""
        è¡¥å…¨è¡¨è¾¾å¼ä¸­çš„ä¸€äº›ç‰¹æ®Šå½¢å¼,å¦‚è¢«çœç•¥çš„ä¹˜å·
        :return: None
        """

        # å®šä¹‰æ•°å­—å’Œæ— ç†æ•°çš„ç±»å‹é›†åˆ
        NUMBERS = {
            Token.TYPE.FINITE_DECIMAL,
            Token.TYPE.INFINITE_DECIMAL,
            Token.TYPE.PERCENTAGE,
            Token.TYPE.INTEGER,
            Token.TYPE.NATIVE_IRRATIONAL,
            Token.TYPE.SHORT_CUSTOM,
            Token.TYPE.LONG_CUSTOM
        }

        IRRATIONALS = {
            Token.TYPE.LONG_CUSTOM,
            Token.TYPE.SHORT_CUSTOM,
            Token.TYPE.NATIVE_IRRATIONAL
        }

        # éå† Token åˆ—è¡¨
        index = 0
        while index < len(self.tokens) - 1:
            current_token = self.tokens[index]
            next_token = self.tokens[index + 1]

            # æƒ…å†µ 1: æ•°å­—åæ¥ (
            if current_token.type in NUMBERS and next_token.type == Token.TYPE.LBRACKET:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # æƒ…å†µ 2: æ— ç†æ•°å‚æ•°åæ¥ (
            elif current_token.type == Token.TYPE.IRRATIONAL_PARAM and next_token.type == Token.TYPE.LBRACKET:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # æƒ…å†µ 3: ) åæ¥æ•°å­—
            elif current_token.type == Token.TYPE.RBRACKET and next_token.type in NUMBERS:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # æƒ…å†µ 4: æ— ç†æ•°åæ¥æ— ç†æ•°
            elif current_token.type in IRRATIONALS and next_token.type in IRRATIONALS:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # æƒ…å†µ 5: æ•°å­—åæ¥æ— ç†æ•°
            elif current_token.type in NUMBERS and next_token.type in IRRATIONALS:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # æƒ…å†µ 6: æ— ç†æ•°åæ¥æ•°å­—
            elif current_token.type in IRRATIONALS and next_token.type in NUMBERS:
                self.tokens = self.tokens[:index + 1] + [
                    Token(Token.TYPE.OPERATOR, "*", [index + 1, index + 2])] + self.tokens[
                                                                               index + 1:]

            # å‰è¿›åˆ°ä¸‹ä¸€ä¸ª Token
            index += 1
        self.tokens, self.expression = Lexer.update(self.tokens)

    def _check_denominator(self, check_tokens: list[Token, Token, Token]) -> list[Token, Token, Token]:
        r"""
        æ£€æŸ¥ä¼ å…¥çš„åˆ†æ•°æµ(åˆ†å­,åˆ†æ•°çº¿,åˆ†æ¯)ä¸­çš„åˆ†æ¯æ˜¯å¦åˆæ³•
        :param check_tokens: è¢«æ£€æŸ¥çš„Token
        :raise OlocInvalidCalculationException: å¦‚æœåˆ†æ¯ä¸º0
        :return: åŸæ ·è¿”å›
        """
        if int(check_tokens[2].value) == 0:
            raise OlocInvalidCalculationException(
                exception_type=OlocInvalidCalculationException.EXCEPTION_TYPE.DIVIDE_BY_ZERO,
                expression=self.expression,
                positions=list(range(*[check_tokens[0].range[0], check_tokens[2].range[1]])),
                computing_unit=check_tokens[0].value + check_tokens[1].value + check_tokens[2].value,
            )
        return check_tokens

    def _fractionalization(self) -> None:
        r"""
        å°†è¡¨è¾¾å¼Tokenæµä¸­çš„å„ç§æ•°å­—è½¬æ¢ä¸ºåˆ†æ•°
        :return: None
        """

        def _convert_finite_decimal(finite_decimal: Token) -> [Token, Token, Token]:
            r"""
            å°†æœ‰é™å°æ•°è½¬ä¸ºåˆ†æ•°
            :param finite_decimal: å¾…è½¬æ¢çš„æœ‰é™å°æ•°
            :return: è½¬æ¢åçš„åˆ†æ•°Tokenæµ(åˆ†å­,åˆ†æ•°çº¿,åˆ†æ¯)
            """
            integer_part, decimal_part = finite_decimal.value.split('.')

            numerator = int(integer_part + decimal_part)
            denominator = 10 ** len(decimal_part)

            if int(integer_part) < 0:
                numerator = -numerator

            fraction = [Token(Token.TYPE.INTEGER,
                              str(numerator),
                              [finite_decimal.range[0], finite_decimal.range[0] + len(str(numerator))]
                              ),
                        Token(Token.TYPE.OPERATOR,
                              "/",
                              [finite_decimal.range[0] + len(str(numerator)) + 1,
                               finite_decimal.range[0] + len(str(numerator)) + 2]
                              ),
                        Token(Token.TYPE.INTEGER,
                              str(denominator),
                              [finite_decimal.range[0] + len(str(numerator)) + 2,
                               finite_decimal.range[0] + len(str(numerator)) + 2 + len(str(denominator))]
                              ),
                        ]

            return fraction

        def _convert_infinite_decimal(infinite_decimal: Token) -> [Token, Token, Token]:
            r"""
            å°†æ— é™å¾ªç¯å°æ•°è½¬ä¸ºåˆ†æ•°
            :param infinite_decimal: å¾…è½¬æ¢çš„æ— é™å°æ•°
            :return: è½¬æ¢åçš„åˆ†æ•°, ä¾æ¬¡ä¸ºåˆ†å­, åˆ†æ•°çº¿, åˆ†æ¯
            """

            def _spilt_decimal_parts(process_decimal: str) -> list[str, str]:
                r"""
                åˆ‡åˆ†æ— é™å¾ªç¯å°æ•°ä¸­é‡å¤çš„éƒ¨åˆ†å’Œæœ‰é™å°æ•°éƒ¨åˆ†
                :param process_decimal: å¾…æŸ¥æ‰¾çš„æ— é™å¾ªç¯å°æ•°
                :return: ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨, ç¬¬ä¸€é¡¹æ˜¯æŸ¥æ‰¾åˆ°çš„é‡å¤éƒ¨åˆ†, ç¬¬äºŒé¡¹æ˜¯ç§»é™¤é‡å¤éƒ¨åˆ†åçš„æœ‰é™å°æ•°
                """
                # å¤„ç†ç»“å°¾æœ‰ç‚¹å·çš„æƒ…å†µ
                if '.' in process_decimal and process_decimal.count('.') > 1:
                    # ç§»é™¤ç»“å°¾çš„æ‰€æœ‰ç‚¹å·
                    base_number = process_decimal.rstrip('.')

                    # åˆ†ç¦»æ•´æ•°å’Œå°æ•°éƒ¨åˆ†
                    integer_part, decimal_part = base_number.split('.')

                    # æœ€åä¸€ä½æ•°å­—æ˜¯å¾ªç¯éƒ¨åˆ†
                    if decimal_part:
                        repeat_part = decimal_part[-1]
                        finite_part = integer_part + "." + decimal_part[:-1]
                    else:
                        # å¦‚æœæ²¡æœ‰å°æ•°éƒ¨åˆ†ï¼Œé»˜è®¤å¾ªç¯éƒ¨åˆ†ä¸º0
                        repeat_part = "0"
                        finite_part = integer_part + ".0"

                    return [repeat_part, finite_part]

                # å¤„ç†æ˜¾å¼å£°æ˜å¾ªç¯éƒ¨åˆ†çš„æƒ…å†µï¼ˆä½¿ç”¨:åˆ†éš”ï¼‰
                else:
                    base_number, repeat_part = process_decimal.split(':')

                    if '.' in base_number:
                        integer_part, decimal_part = base_number.split('.')
                        finite_part = integer_part + "." + decimal_part
                    else:
                        # å¦‚æœåŸºæ•°éƒ¨åˆ†æ²¡æœ‰å°æ•°ç‚¹ï¼ŒåŠ ä¸Š.0
                        finite_part = base_number + ".0"

                    return [repeat_part, finite_part]

            def _fraction_from_parts(repeat_part: str, finite_part: str) -> list[Token, Token, Token]:
                r"""
                æ ¹æ®å¾ªç¯éƒ¨åˆ†å’Œæœ‰é™éƒ¨åˆ†è®¡ç®—åˆ†æ•°å½¢å¼
                :param repeat_part: å¾ªç¯éƒ¨åˆ†
                :param finite_part: æœ‰é™éƒ¨åˆ†
                :return: è½¬æ¢åçš„åˆ†æ•°Tokenæµ(åˆ†å­,åˆ†æ•°çº¿,åˆ†æ¯)
                """
                # åˆ†è§£æœ‰é™éƒ¨åˆ†
                if '.' in finite_part:
                    integer_str, decimal_str = finite_part.split('.')
                else:
                    integer_str, decimal_str = finite_part, '0'

                # å°†æ•´æ•°éƒ¨åˆ†è½¬ä¸ºæ•´æ•°
                integer_value = int(integer_str) if integer_str else 0

                # è®¡ç®—åˆ†æ¯ï¼šå¾ªç¯éƒ¨åˆ†äº§ç”Ÿçš„åˆ†æ¯æ˜¯9çš„ä¹˜ç§¯
                denominator = int('9' * len(repeat_part))

                # å¦‚æœæœ‰é™å°æ•°éƒ¨åˆ†éç©ºï¼Œéœ€è¦å°†å¾ªç¯éƒ¨åˆ†ä¹˜ä»¥é€‚å½“çš„å› å­
                if decimal_str:
                    denominator = denominator * (10 ** len(decimal_str))

                # è®¡ç®—åˆ†å­
                numerator = 0

                # å¤„ç†æ•´æ•°éƒ¨åˆ†
                if integer_value:
                    numerator += integer_value * denominator

                # å¤„ç†æœ‰é™å°æ•°éƒ¨åˆ†
                if decimal_str:
                    numerator += int(decimal_str) * int('9' * len(repeat_part))

                # å¤„ç†å¾ªç¯éƒ¨åˆ†
                if repeat_part:
                    numerator += int(repeat_part)

                # è¿”å›åˆ†æ•°å½¢å¼
                fraction = [Token(Token.TYPE.INTEGER,
                                  str(numerator),
                                  [infinite_decimal.range[0], infinite_decimal.range[0] + len(str(numerator))]
                                  ),
                            Token(Token.TYPE.OPERATOR,
                                  "/",
                                  [infinite_decimal.range[0] + len(str(numerator)) + 1,
                                   infinite_decimal.range[0] + len(str(numerator)) + 2]
                                  ),
                            Token(Token.TYPE.INTEGER,
                                  str(denominator),
                                  [infinite_decimal.range[0] + len(str(numerator)) + 2,
                                   infinite_decimal.range[0] + len(str(numerator)) + 2 + len(str(denominator))]
                                  ),
                            ]
                return fraction

            # ä¸»å‡½æ•°é€»è¾‘
            infinite_decimal_str = infinite_decimal.value
            parts = _spilt_decimal_parts(infinite_decimal_str)
            repeat_part, finite_part = parts[0], parts[1]

            # è®¡ç®—åˆ†æ•°
            fraction = _fraction_from_parts(repeat_part, finite_part)

            # è°ƒç”¨åŒ–ç®€å‡½æ•°
            return fraction

        def _convert_percentage(percentage: Token) -> [Token, Token, Token]:
            r"""
            å°†ç™¾åˆ†æ•°è½¬ä¸ºå°æ•°
            :param percentage: å¾…è½¬æ¢çš„ç™¾åˆ†æ•°
            :return: è½¬æ¢åçš„å°æ•°å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚"0.125"
            """
            # å»æ‰ç™¾åˆ†å·
            percentage_str = percentage.value
            percentage_str = percentage_str[:-1]

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å°æ•°ç‚¹ï¼Œç¡®ä¿åˆ†å‰²æ“ä½œä¸ä¼šå‡ºé”™
            if '.' not in percentage_str:
                percentage_str += '.0'

            integer_part, decimal_part = percentage_str.split('.')

            # æ ¹æ®æ•´æ•°éƒ¨åˆ†é•¿åº¦è°ƒæ•´å°æ•°ç‚¹ä½ç½®
            if integer_part == '0':
                percentage_str = '0.00' + decimal_part
            elif len(integer_part) == 1:
                percentage_str = '0.0' + integer_part + decimal_part
            elif len(integer_part) == 2:
                percentage_str = '0.' + integer_part + decimal_part
            else:
                decimal_point_pos = len(integer_part) - 2
                percentage_str = integer_part[:decimal_point_pos] + '.' + integer_part[decimal_point_pos:] + decimal_part

            percentage_str = percentage_str.rstrip('0')
            if percentage_str.endswith('.'):
                percentage_str = percentage_str[:-1]

            return [Token(Token.TYPE.INTEGER, percentage_str, [percentage.range[0], percentage.range[0] + len(percentage_str)]),
                    Token(Token.TYPE.OPERATOR, "/", [percentage.range[0] + len(percentage_str), percentage.range[0] + len(percentage_str) + 1]),
                    Token(Token.TYPE.INTEGER, "1",
                          [percentage.range[0] + len(percentage_str) + 1, percentage.range[0] + len(percentage_str) + 2]),
                    ] if '.' not in percentage_str else _convert_finite_decimal(Token(Token.TYPE.FINITE_DECIMAL, percentage_str, [percentage.range[0], percentage.range[0] + len(percentage_str)]))

        fractionalized_tokens = []

        convert_num_types = [
            Token.TYPE.PERCENTAGE,
            Token.TYPE.FINITE_DECIMAL,
            Token.TYPE.INFINITE_DECIMAL,
        ]

        tokens_to_fractionalized: list[Token] = []
        index = 0
        while index < len(self.tokens):
            temp_token = self.tokens[index]
            if (convert_type := temp_token.type) == Token.TYPE.INTEGER and \
                    index + 2 < len(self.tokens) and \
                    self.tokens[index + 1].value == "/" and \
                    self.tokens[index + 2].type == Token.TYPE.INTEGER:
                fractionalized_tokens += Evaluator.simplify(
                    self._check_denominator([temp_token, self.tokens[index + 1], self.tokens[index + 2]]))
                index += 3
            elif convert_type in convert_num_types:
                match convert_type:
                    case Token.TYPE.FINITE_DECIMAL:
                        tokens_to_fractionalized = self._check_denominator(_convert_finite_decimal(temp_token))
                    case Token.TYPE.INFINITE_DECIMAL:
                        tokens_to_fractionalized = self._check_denominator(_convert_infinite_decimal(temp_token))
                    case Token.TYPE.PERCENTAGE:
                        tokens_to_fractionalized = self._check_denominator(_convert_percentage(temp_token))
                fractionalized_tokens += Evaluator.simplify(tokens_to_fractionalized)
            else:
                fractionalized_tokens += [temp_token]
            index += 1

        self.tokens = fractionalized_tokens
        self.tokens, self.expression = Lexer.update(self.tokens)

    def _bracket_checking_harmonisation(self) -> None:
        """
        æ‹¬å·æ£€æŸ¥ä¸ç»Ÿä¸€åŒ–
        :raise OlocInvalidBracketException: å¦‚æœæ‹¬å·å‡ºç°å±‚çº§é”™è¯¯æˆ–ä¸åŒ¹é…
        :return: None
        """
        # å®šä¹‰æ‹¬å·çš„å±‚çº§ä¼˜å…ˆçº§
        BRACKET_PRIORITY = {'(': 1, '[': 2, '{': 3, ')': 1, ']': 2, '}': 3}
        # å®šä¹‰æ‹¬å·çš„å·¦å³åŒ¹é…å…³ç³»
        BRACKET_MATCH = {'(': ')', '[': ']', '{': '}', ')': '(', ']': '[', '}': '{'}

        # æ ˆç»“æ„ç”¨äºåŒ¹é…æ‹¬å·
        stack = []

        for temp_token in self.tokens:
            if temp_token.type == Token.TYPE.LBRACKET:  # å·¦æ‹¬å·
                # æ£€æŸ¥å±‚çº§æ˜¯å¦åˆæ³•
                if stack and BRACKET_PRIORITY[temp_token.value] > BRACKET_PRIORITY[stack[-1][0]]:
                    raise OlocInvalidBracketException(
                        exception_type=OlocInvalidBracketException.EXCEPTION_TYPE.INCORRECT_BRACKET_HIERARCHY,
                        expression=self.expression,
                        positions=[temp_token.range[0]],
                        err_bracket=temp_token.value
                    )

                # å°†å·¦æ‹¬å·å‹å…¥æ ˆ (æ‹¬å·å€¼, èµ·å§‹ä½ç½®, ä¼˜å…ˆçº§)
                stack.append((temp_token.value, temp_token.range[0], BRACKET_PRIORITY[temp_token.value]))

            elif temp_token.type == Token.TYPE.RBRACKET:  # å³æ‹¬å·
                # æ ˆä¸ºç©ºæ—¶ï¼Œå·¦æ‹¬å·ç¼ºå¤±
                if not stack:
                    raise OlocInvalidBracketException(
                        exception_type=OlocInvalidBracketException.EXCEPTION_TYPE.MISMATCH_RIGHT_BRACKET,
                        expression=self.expression,
                        positions=[temp_token.range[0]],
                        err_bracket=temp_token.value
                    )

                # å¼¹å‡ºæ ˆé¡¶å…ƒç´ 
                last_left_bracket, last_position, last_priority = stack.pop()

                # æ£€æŸ¥æ‹¬å·æ˜¯å¦åŒ¹é…
                if BRACKET_MATCH[last_left_bracket] != temp_token.value:
                    raise OlocInvalidBracketException(
                        exception_type=OlocInvalidBracketException.EXCEPTION_TYPE.MISMATCH_LEFT_BRACKET,
                        expression=self.expression,
                        positions=[last_position],
                        err_bracket=last_left_bracket
                    )

                # æ£€æŸ¥å±‚çº§æ˜¯å¦åˆæ³•
                if BRACKET_PRIORITY[last_left_bracket] != BRACKET_PRIORITY[temp_token.value]:
                    raise OlocInvalidBracketException(
                        exception_type=OlocInvalidBracketException.EXCEPTION_TYPE.INCORRECT_BRACKET_HIERARCHY,
                        expression=self.expression,
                        positions=[last_position],
                        err_bracket=last_left_bracket
                    )

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªåŒ¹é…çš„å·¦æ‹¬å·
        if stack:
            last_left_bracket, last_position, _ = stack.pop()
            raise OlocInvalidBracketException(
                exception_type=OlocInvalidBracketException.EXCEPTION_TYPE.MISMATCH_LEFT_BRACKET,
                expression=self.expression,
                positions=[last_position],
                err_bracket=last_left_bracket
            )

        for bracket_token in self.tokens:
            if bracket_token.type == Token.TYPE.LBRACKET:
                bracket_token.value = '('
            elif bracket_token.type == Token.TYPE.RBRACKET:
                bracket_token.value = ')'

        Lexer.update(self.tokens)

    def _function_conversion(self) -> None:
        r"""
        æ ¹æ®å‡½æ•°è½¬æ¢è¡¨ï¼Œå°†è¡¨è¾¾å¼ä¸­çš„å³ä¾§å½¢å¼æ›¿æ¢ä¸ºå·¦ä¾§æ ‡å‡†å½¢å¼
        :return: None
        """

        function_conversion_table = utils.get_function_conversion_table()

        class Expression:
            r"""
            è¡¨è¾¾å¼å­å•å…ƒ
            :param tokens_to_build: æ„é€ è¯¥è¡¨è¾¾å¼éƒ¨åˆ†çš„å­å•å…ƒ
            """

            def __init__(self, tokens_to_build: list[Token]):
                self.tokens = tokens_to_build

            def __repr__(self):
                return f"Expression: {self.tokens}"

        def _build_match(match_case: str) -> list[Token | Expression]:
            r"""
            æ ¹æ®éœ€è¦åŒ¹é…çš„å­—ç¬¦ä¸²å½¢å¼æ„é€ å¯¹åº”åŒ¹é…æ¨¡å¼çš„åˆ—è¡¨
            :param match_case: éœ€è¦åŒ¹é…çš„å­—ç¬¦ä¸²å½¢å¼çš„æ¨¡å¼
            :return: ä¸€ä¸ªåˆ—è¡¨,å¯¹åº”éœ€è¦åŒ¹é…çš„æ¨¡å¼
            """
            match = Lexer.tokenizer(match_case)
            result = []
            for temp_token in match:
                if temp_token.type == Token.TYPE.LONG_CUSTOM and temp_token.value in ['<__reserved_param1__>', '<__reserved_param2__>']:
                    result.append(Expression([temp_token]))
                else:
                    result.append(temp_token)
            return result

        def _find_match(match_case: list[Token | Expression], token_list: list[Token | Expression]) -> list[bool, list[int, int]]:
            r"""
            æ‰¾åˆ°Tokenæµä¸­å’ŒåŒ¹é…æ¨¡å¼åŒ¹é…çš„éƒ¨åˆ†
            :param match_case: éœ€è¦åŒ¹é…çš„æµå½¢å¼
            :param token_list: å¾…åŒ¹é…çš„æ¨¡å¼æµ
            :return: ä¸€ä¸ªåˆ—è¡¨.ç¬¬ä¸€é¡¹æ˜¯æ˜¯å¦åŒ¹é…åˆ°ç»“æœ,ç¬¬äºŒé¡¹æ˜¯ä¸¤ä¸ªå…ƒç´ çš„æ•´æ•°åˆ—è¡¨,å¯¹åº”èŒƒå›´çš„ä¸‹æ ‡.
            """
            def _is_match(units_of_list: list[Token | Expression], units_of_match: list[Token | Expression]) -> bool:
                r"""
                åˆ¤æ–­å¯¹åº”å•å…ƒæ˜¯å¦åŒ¹é…
                :param units_of_list: å¾…åŒ¹é…æµä¸­çš„å•å…ƒå—(åˆ—è¡¨)
                :param units_of_match: æ¨¡å¼æµä¸­çš„å•å…ƒå—(åˆ—è¡¨)
                :return: æ˜¯å¦åŒ¹é…
                """
                for list_unit, match_unit in zip(units_of_list, units_of_match):
                    if isinstance(list_unit, Expression) and isinstance(match_unit, Expression):
                        continue
                    if isinstance(list_unit, Token) and isinstance(match_unit, Token) and list_unit.type == match_unit.type:
                        # è‡ªå®šä¹‰çŸ­/é•¿æ— ç†æ•°: ä¸éœ€è¦å†…å®¹ä¸€è‡´
                        if list_unit.type in [Token.TYPE.SHORT_CUSTOM, Token.TYPE.LONG_CUSTOM] and match_unit.type in [Token.TYPE.SHORT_CUSTOM, Token.TYPE.LONG_CUSTOM]:
                            continue
                    if not list_unit.value == match_unit.value:
                        break
                else:
                    return True
                return False

            for index, unit in enumerate(token_list):
                if unit == match_case[0] and (attempt_length := index + len(match_case)) <= len(token_list):
                    if _is_match(token_list[index:attempt_length], match_case):
                        return [True, [index, attempt_length]]
            return [False, [0, 0]]

        def _convert_match(to_convert: list[Token | Expression], match_pattern: list[Token | Expression], match_range: [int, int]) -> list[Token]:
            r"""
            å°†æ‰¾åˆ°çš„åŒ¹é…ç»“æ„è½¬æ¢ä¸ºconvert_toçš„ç»“æ„, å¹¶è§£å¼€Expression
            :param to_convert: å¾…ä¿®æ”¹çš„Token | Expressionæµ
            :param match_pattern: åŒ¹é…çš„Token | Expressionæµ
            :param match_range: è¢«åŒ¹é…çš„æ¨¡å¼èŒƒå›´
            :return:
            """
            def _unwrap_to_token_list(process_list: list[Token | Expression]) -> list[Token]:
                r"""
                å°†Token | Expressionæµè½¬æ¢ä¸ºTokenæµ
                :param process_list: å¾…è½¬æ¢çš„Token | Expressionæµ
                :return: è½¬æ¢åçš„Tokenæµ
                """
                unwrap_result = []
                for process_unit in process_list:
                    if isinstance(process_unit, Expression):
                        unwrap_result.extend(process_unit.tokens)
                    else:
                        unwrap_result.append(process_unit)
                return unwrap_result

            result = to_convert[:match_range[0]]

            params = [unit for unit in to_convert[match_range[0]:match_range[1]] if isinstance(unit, Expression)]

            param_index = 0
            after_convert = []
            for temp_unit in match_pattern:
                if isinstance(temp_unit, Expression):
                    after_convert.append(params[param_index])
                    param_index += 1
                after_convert.append(temp_unit)

            result += after_convert + to_convert[match_range[1]:]

            return _unwrap_to_token_list(result)

        def _has_convert(token_list: list[Token], matches_to_judge: list[list[Token]]) -> bool:
            r"""
            åˆ¤æ–­Tokenæµä¸­æ˜¯å¦å­˜åœ¨å¯è½¬æ¢çš„ç»“æ„
            :param token_list: å¾…è½¬æ¢çš„ç»“æ„
            :param matches_to_judge: å¾…åˆ¤æ–­çš„ç¼“å­˜åŒ¹é…
            :return: æ˜¯å¦å­˜åœ¨
            """
            for temp_match in matches_to_judge:
                if _find_match(temp_match, token_list):
                    return True
            return False

        def _build_expression_token_list(token_list: list[Token]) -> list[Token | Expression]:
            r"""
            å°†Tokenæµä¸­è¡¨è¾¾å¼éƒ¨åˆ†è½¬æ¢ä¸ºExpression
            :param token_list: å¾…è½¬æ¢çš„Tokenæµ
            :return: è½¬æ¢åçš„Token | Expressionæµ. å¦‚æœè¾“å…¥å’Œè¾“å‡ºä¸€è‡´,è¯´æ˜ä¸å†æœ‰å¯ä»¥è½¬æ¢çš„éƒ¨åˆ†äº†.
            """
            for function_strs in utils.get_function_conversion_table().values():
                for function_str in function_strs:
                    ...
            return token_list

        matches = []
        for key, value_list in function_conversion_table.items():
            for value in value_list:
                matches.append(_build_match(value))

        while True:
            expression_list = _build_expression_token_list(self.tokens)
            for match_case in matches:
                is_find, find_range = _find_match(match_case, expression_list)
                if is_find:
                    self.tokens = _convert_match(expression_list, match_case, find_range)
                    Lexer.update(self.tokens)
            if not _has_convert(self.tokens, matches):
                break

    def _static_check(self) -> None:
        r"""
        å¯¹è¡¨è¾¾å¼æ‰§è¡Œé™æ€æ£€æŸ¥
        :return: None
        """

    def execute(self):
        r"""
        æ‰§è¡Œåˆ†è¯å™¨
        :return: None
        """

        start_time = time.time_ns()
        self._convert_token_flow()
        self._formal_complementation()
        self._fractionalization()
        self._bracket_checking_harmonisation()
        self._function_conversion()
        self.time_cost = time.time_ns() - start_time

    """
    é™æ€æ–¹æ³•
    """

    @staticmethod
    def tokenizer(expression: str) -> list[Token]:
        r"""
        åˆ†è¯å™¨
        :param expression: å¾…åˆ†è¯çš„è¡¨è¾¾å¼
        :raise OlocIrrationalNumberException: å¦‚æœæ— ç†æ•°å½¢å¼ä¸åˆæ³•
        :return: åˆ†è¯åçš„Tokenåˆ—è¡¨
        """
        function_names = utils.get_function_name_list()
        symbol_mapping_table = utils.get_symbol_mapping_table()

        mark_list = [Token.TYPE.UNKNOWN for _ in range(len(expression))]

        """
        æ¨¡å—æ ‡è®°
        """

        # æ ‡è®°è‡ªå®šä¹‰é•¿æ— ç†æ•°
        for index, char in enumerate(expression):
            # å·²ç»æ ‡è®°è¿‡çš„è·³è¿‡

            if index > 0 and mark_list[index - 1] == Token.TYPE.LONG_CUSTOM:
                continue

            if char == '<':

                # å•ä¸ªå·¦å°–æ‹¬å·æ˜¯é”™è¯¯çš„
                if index == len(expression) - 1:
                    raise OlocIrrationalNumberException(
                        exception_type=OlocIrrationalNumberException.EXCEPTION_TYPE.MISMATCH_LONG_LEFT_SIGN,
                        expression=expression,
                        positions=[index, index],
                    )

                # æŸ¥æ‰¾åŒ¹é…çš„å³å°–æ‹¬å·
                right_bracket_index = None
                for i in range(index + 1, len(expression)):
                    if expression[i] == '>':
                        right_bracket_index = i
                        break

                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„å³å°–æ‹¬å·ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if right_bracket_index is None:
                    raise OlocIrrationalNumberException(
                        exception_type=OlocIrrationalNumberException.EXCEPTION_TYPE.MISMATCH_LONG_LEFT_SIGN,
                        expression=expression,
                        positions=[index, index],
                    )

                # æ ‡è®°æ•´ä¸ªèŒƒå›´ä¸ºLONG_CUSTOM
                for i in range(index, right_bracket_index + 1):
                    mark_list[i] = Token.TYPE.LONG_CUSTOM

        # æ ‡è®°æ— ç†æ•°å‚æ•°
        for index, char in enumerate(expression):
            if char == "?":
                # è®°å½•å½“å‰ ? çš„ç´¢å¼•å¹¶åˆå§‹åŒ–ç´¢å¼•åˆ—è¡¨
                irrational_param_index_list = [index]

                # å¼€å§‹å‘å‰æ‰«æ
                find_dot = False

                for scan_index in range(index - 1, -1, -1):  # ä»å½“å‰ç´¢å¼•å‘å‰éå†
                    scan_char = expression[scan_index]

                    if scan_char.isdigit() or scan_char in {".", "+", "-"}:
                        irrational_param_index_list.append(scan_index)
                        if scan_char == ".":
                            if find_dot:  # å¦‚æœå·²ç»é‡åˆ°è¿‡å°æ•°ç‚¹ï¼Œåœæ­¢æ‰«æ
                                break
                            find_dot = True
                        elif scan_char in {"+", "-"}:  # é‡åˆ°åŠ å·æˆ–å‡å·ï¼Œåœæ­¢æ‰«æ
                            break
                    else:  # éæ•°å­—ã€éç¬¦å·ç›´æ¥åœæ­¢
                        break

                # æ ‡è®°æ‰€æœ‰ç›¸å…³ç´¢å¼•ä¸º IRRATIONAL_PARAM
                for irrational_index in irrational_param_index_list:
                    mark_list[irrational_index] = Token.TYPE.IRRATIONAL_PARAM

        for func_name in function_names:
            start = 0
            func_len = len(func_name)

            # åœ¨è¡¨è¾¾å¼ä¸­æŸ¥æ‰¾å‡½æ•°å
            while (index := expression.find(func_name, start)) != -1:
                end_index = index + func_len
                # æ ‡è®°åŒ¹é…èŒƒå›´å†…çš„å­—ç¬¦ä¸º FUNCTIONï¼Œä¸å†æ£€æŸ¥å‰åå­—ç¬¦
                for i in range(index, end_index):
                    mark_list[i] = Token.TYPE.FUNCTION

                # æ›´æ–°æŸ¥æ‰¾çš„èµ·å§‹ä½ç½®ï¼Œé¿å…é‡å¤åŒ¹é…
                start = end_index

        # æ ‡è®°æ•°å­—
        for index, char in enumerate(expression):

            # å¦‚æœå½“å‰ç´¢å¼•å·²ç»å¤„ç†è¿‡ï¼Œåˆ™è·³è¿‡
            if mark_list[index] != Token.TYPE.UNKNOWN:
                continue

            # å¤„ç†æ•°å­—
            if char.isdigit():
                mode = Token.TYPE.INTEGER  # é»˜è®¤æ¨¡å¼ä¸ºæ•´æ•°
                attempt_index = index
                digit_index_range_list = [index]  # ä¿å­˜æ•°å­—ç´¢å¼•èŒƒå›´
                find_decimal_point = False  # æ˜¯å¦æ‰¾åˆ°å°æ•°ç‚¹
                is_infinite_decimal = False  # æ˜¯å¦ä¸ºæ— é™å°æ•°

                while attempt_index + 1 < len(expression):
                    attempt_index += 1
                    current_char = expression[attempt_index]

                    if current_char == "%" and mode in [Token.TYPE.INTEGER, Token.TYPE.FINITE_DECIMAL]:
                        if attempt_index + 1 < len(expression) and expression[attempt_index + 1] not in ["+", "-", "*",
                                                                                                         "/",
                                                                                                         "^", "%", "|",
                                                                                                         ")", "]", "}",
                                                                                                         ",", ";"]:
                            break
                        mode = Token.TYPE.PERCENTAGE
                        digit_index_range_list.append(attempt_index)
                        break

                    # å¤„ç†å°æ•°ç‚¹
                    if current_char == ".":
                        if find_decimal_point:  # å¦‚æœå·²ç»æ‰¾åˆ°è¿‡å°æ•°ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ— é™å°æ•°
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ— é™å¾ªç¯å°æ•°
                            next_char_index = attempt_index + 1
                            if next_char_index < len(expression) and expression[next_char_index] == ".":
                                is_infinite_decimal = True
                                mode = Token.TYPE.INFINITE_DECIMAL
                                digit_index_range_list.append(attempt_index)  # å°†å½“å‰ç‚¹åŠ å…¥æ ‡æ³¨èŒƒå›´
                                param_index = attempt_index + 1
                                point_count = 1  # è®°å½•è¿ç»­ç‚¹çš„æ•°é‡

                                while param_index < len(expression):
                                    if expression[param_index] == ".":
                                        point_count += 1
                                        digit_index_range_list.append(param_index)  # å°†ç‚¹åŠ å…¥æ ‡æ³¨èŒƒå›´
                                    else:
                                        break  # é‡åˆ°éç‚¹å­—ç¬¦æ—¶é€€å‡º
                                    param_index += 1

                                if not 6 <= point_count <= 3:
                                    break

                                # ç¡®ä¿æœ€åä¸€ä¸ªç‚¹ä¹Ÿè¢«æ ‡è®°
                                attempt_index = param_index - 1
                                continue
                            else:
                                break  # å¦‚æœä¸æ˜¯æ— é™å°æ•°ï¼Œé€€å‡º
                        find_decimal_point = True
                        mode = Token.TYPE.FINITE_DECIMAL
                        digit_index_range_list.append(attempt_index)  # å°†å°æ•°ç‚¹åŠ å…¥æ ‡æ³¨èŒƒå›´
                        continue

                    # å¤„ç†æ˜¾å¼æ ‡æ³¨çš„æ— ç†æ•°
                    if current_char == ":" and find_decimal_point:
                        is_infinite_decimal = True
                        digit_index_range_list.append(attempt_index)
                        next_index = attempt_index + 1
                        while next_index < len(expression):
                            if not expression[next_index].isdigit():
                                break
                            digit_index_range_list.append(next_index)
                            next_index += 1

                    # å¤„ç†æ•°å­—å­—ç¬¦
                    if current_char.isdigit():
                        digit_index_range_list.append(attempt_index)
                    else:
                        break

                # å¦‚æœå½“å‰æ˜¯æ— é™å°æ•°ï¼Œå°†æ‰€æœ‰æ ‡è®°ä¸º INFINITE_DECIMAL
                if is_infinite_decimal:
                    mode = Token.TYPE.INFINITE_DECIMAL

                # æ›´æ–°æ ‡è®°åˆ—è¡¨
                for digit_index in digit_index_range_list:
                    if 0 <= digit_index < len(mark_list):  # ç¡®ä¿ç´¢å¼•åˆæ³•
                        mark_list[digit_index] = mode

        r"""
        é€å­—ç¬¦æ‰«æ
        """
        # é€å­—ç¬¦æ‰«æ
        for index, unit in enumerate(zip(expression, mark_list)):
            unit_char: str = unit[0]
            unit_type = unit[1]

            # å·²ç»æ ‡è®°è¿‡
            if unit_type != Token.TYPE.UNKNOWN:
                continue

            # æ ‡è®°å‡½æ•°å‚æ•°åˆ†éš”ç¬¦
            if unit_char in ";,":
                mark_list[index] = Token.TYPE.PARAM_SEPARATOR

            # æ ‡è®°æ‹¬å·
            if unit_char in "{[(":
                mark_list[index] = Token.TYPE.LBRACKET
                continue
            if unit_char in ")]}":
                mark_list[index] = Token.TYPE.RBRACKET
                continue

            # æ ‡è®°éæ•°å­—
            if unit_char in ["Ï€", "ğ‘’"]:
                mark_list[index] = Token.TYPE.NATIVE_IRRATIONAL
                continue

            if unit_char in symbol_mapping_table.keys():
                mark_list[index] = Token.TYPE.OPERATOR
                continue
            else:
                mark_list[index] = Token.TYPE.SHORT_CUSTOM
                continue

        """
        åˆå¹¶Token
        """
        tokens = []  # æœ€ç»ˆç”Ÿæˆçš„ Token åˆ—è¡¨
        current_type = None  # å½“å‰æ­£åœ¨å¤„ç†çš„ Token ç±»å‹
        current_value = ""  # å½“å‰æ­£åœ¨æ„é€ çš„ Token å€¼
        current_start = 0  # å½“å‰ Token çš„èµ·å§‹ç´¢å¼•

        # éœ€è¦åˆå¹¶çš„ Token ç±»å‹
        mergeable_types = {
            Token.TYPE.PERCENTAGE,
            Token.TYPE.INFINITE_DECIMAL,
            Token.TYPE.FINITE_DECIMAL,
            Token.TYPE.INTEGER,
            Token.TYPE.LONG_CUSTOM,
            Token.TYPE.IRRATIONAL_PARAM,
            Token.TYPE.FUNCTION,
            Token.TYPE.UNKNOWN,
        }

        for i, (char, token_type) in enumerate(zip(expression, mark_list)):
            if current_type is None:  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ª Token
                current_type = token_type
                current_value = char
                current_start = i
            elif token_type == current_type and token_type in mergeable_types:  # å¦‚æœç±»å‹ç›¸åŒä¸”å¯åˆå¹¶ï¼Œç»§ç»­åˆå¹¶
                current_value += char
            else:  # é‡åˆ°ä¸åŒç±»å‹çš„ Tokenï¼Œä¿å­˜å½“å‰ Tokenï¼Œå¹¶å¼€å§‹æ–°çš„ Token
                tokens.append(Token(current_type, current_value, [current_start, i]))
                current_type = token_type
                current_value = char
                current_start = i

        # å¤„ç†æœ€åä¸€ä¸ª Token
        if current_type is not None:
            tokens.append(Token(current_type, current_value, [current_start, len(expression)]))

        return tokens

    @staticmethod
    def update(tokens: list[Token]) -> [list[Token], str]:
        r"""
        æ›´æ–°è¾“å…¥çš„Tokenæµ
        :return: ä¸€ä¸ªåˆ—è¡¨,ç¬¬ä¸€é¡¹æ˜¯æ›´æ–°åçš„Tokenæµ,ç¬¬äºŒé¡¹æ˜¯è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # æ¸…ç©ºè¡¨è¾¾å¼å’Œèµ·å§‹ä¸‹æ ‡
        expression = ""
        start_index = 0

        # éå†æ‰€æœ‰Tokenï¼Œæ‹¼æ¥è¡¨è¾¾å¼å¹¶æ£€æŸ¥ä¸‹æ ‡è¿ç»­æ€§
        for index, process_token in enumerate(tokens):
            # æ‹¼æ¥è¡¨è¾¾å¼
            expression += process_token.value

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªTokenï¼Œç›´æ¥è®¾ç½®å…¶ä¸‹æ ‡
            if index == 0:
                process_token.range = [start_index, start_index + len(process_token.value)]
                start_index = process_token.range[1]
                continue

            # æ£€æŸ¥å½“å‰Tokenå’Œå‰ä¸€ä¸ªTokençš„ä¸‹æ ‡è¿ç»­æ€§
            previous_token = tokens[index - 1]
            if previous_token.range[1] != process_token.range[0]:
                # ä¸‹æ ‡ä¸è¿ç»­ï¼Œé‡æ–°åˆ†é…å½“å‰TokenåŠåç»­Tokençš„ä¸‹æ ‡
                process_token.range = [start_index, start_index + len(process_token.value)]
            else:
                # ä¸‹æ ‡è¿ç»­ï¼Œä¿æŒå½“å‰ä¸‹æ ‡
                process_token.range = [previous_token.range[1], previous_token.range[1] + len(process_token.value)]

            # æ›´æ–°èµ·å§‹ä¸‹æ ‡
            start_index = process_token.range[1]
        return [tokens, expression]


"""test"""
if __name__ == '__main__':
    import oloc_preprocessor as preprocessor

    import simpsave as ss

    tests = ss.read("test_cases", file="./data/olocconfig.ini")
    # input(f"{len(tests)}>>>")
    start = time.time()
    for test in tests:
        try:
            preprocess = preprocessor.Preprocessor(test)
            preprocess.execute()
            #print(test, end=" => ")
            lexer = Lexer(preprocess.expression)
            lexer.execute()
            for token in lexer.tokens:
                ... # debug
                #print(token.value, end=" ")
            #print(f"\t {preprocess.time_cost / 1000000} ms {lexer.time_cost / 1000000} ms")
        except (TypeError, ZeroDivisionError) as t_error:
            raise t_error
        except Exception as error:
            print(f"\n\n\n========\n\n{error}\n\n\n")
    print(f"Run {len(tests)} in {time.time() - start}")

    while True:
        try:
            preprocess = preprocessor.Preprocessor(input(">>>"))
            preprocess.execute()
            lexer = Lexer(preprocess.expression)
            lexer.execute()
            print(lexer.tokens)
            for token in lexer.tokens:
                print(token.value, end=" ")
            print(f"\nIn {preprocess.time_cost / 1000000000} + {lexer.time_cost / 1000000000} = {preprocess.time_cost + lexer.time_cost} s")
        except (TypeError, ZeroDivisionError) as t_error:
            raise t_error
        except Exception as error:
            print(error)

    preprocess = preprocessor.Preprocessor(input(">>>"))
    preprocess.execute()
    lexer = Lexer(preprocess.expression)
    lexer.execute()
    print(lexer.tokens)
    for token in lexer.tokens:
        print(token.value, end=" ")