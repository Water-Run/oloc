r"""
:author: WaterRun
:date: 2025-04-08
:file: oloc_result.py
:description: Oloc result
"""

import time

from typing import Any
from fractions import Fraction

from oloc_token import Token
from oloc_exceptions import *
from oloc_preprocessor import Preprocessor
from oloc_lexer import Lexer
from oloc_parser import Parser
from oloc_evaluator import Evaluator

import oloc_utils as utils


def output_filter(tokens: list[Token]) -> str:
    r"""
    æ ¼å¼åŒ–è¿‡æ»¤Tokenæµå¹¶è¾“å‡º
    :param tokens: å¾…è¿‡æ»¤è¾“å‡ºçš„tokenæµ
    :return: è¿‡æ»¤åçš„ç”Ÿæˆçš„è¡¨è¾¾å¼å­—ç¬¦ä¸²
    """

    def _add_separator(num: Token, separator: str, thresholds: int, interval: int) -> str:
        r"""
        æ·»åŠ æ•°å­—åˆ†éš”ç¬¦
        :param num: å¾…æ·»åŠ çš„æ•°å­—ï¼ˆToken ç±»å‹ï¼Œéœ€è¦æœ‰ num.value å±æ€§ï¼‰
        :param separator: åˆ†éš”ç¬¦å½¢å¼
        :param thresholds: åˆ†éš”ç¬¦é˜ˆå€¼ï¼ˆå¤§äºè¯¥å€¼æ‰æ·»åŠ åˆ†éš”ç¬¦ï¼‰
        :param interval: åˆ†éš”ç¬¦é—´éš”ï¼ˆæ¯éš” interval ä¸ªå­—ç¬¦æ’å…¥ä¸€ä¸ªåˆ†éš”ç¬¦ï¼‰
        :return: æ·»åŠ åˆ†éš”ç¬¦åçš„å­—ç¬¦ä¸²æ•°å­—
        """
        # å¦‚æœæ•°å­—é•¿åº¦å°äºç­‰äºé˜ˆå€¼ï¼Œç›´æ¥è¿”å›åŸå§‹å€¼
        if len(num.value) <= thresholds:
            return num.value

        # å€’åºæ’å…¥åˆ†éš”ç¬¦ï¼ˆä»å³å‘å·¦æ“ä½œï¼‰
        reversed_num = num.value[::-1]  # å°†å­—ç¬¦ä¸²æ•°å­—åè½¬
        parts = [
            reversed_num[i:i + interval] for i in range(0, len(reversed_num), interval)
        ]  # æŒ‰é—´éš”åˆ‡åˆ†ä¸ºå—
        after_add = separator.join(parts)  # ä½¿ç”¨åˆ†éš”ç¬¦è¿æ¥
        return after_add[::-1]  # å†æ¬¡åè½¬å›åŸå§‹é¡ºåº

    configs = utils.get_formatting_output_function_options_table()

    between_token = " " * configs["readability"]["space between tokens"]
    number_seperator = "_" if configs["custom"]["underline-style number separator"] else ","

    ascii_native_irrational_map = {"Ï€": "pi", "ğ‘’": "e"}
    superscript_map = {'1': 'Â¹', '2': 'Â²', '3': 'Â³', '4': 'â´', '5': 'âµ', '6': 'â¶', '7': 'â·', '8': 'â¸', '9': 'â¹',
                       '0': 'â°'}

    result = ""
    add_superscript = False

    # å­—ç¬¦ä¸²å¤„ç†
    for index, temp_token in enumerate(tokens):

        if temp_token.type != Token.TYPE.INTEGER:
            add_superscript = False
        if configs["readability"]["superscript"] and temp_token.type == Token.TYPE.OPERATOR and temp_token.value == "^":
            add_superscript = True
            continue

        if 1 <= index <= len(tokens) - 1 and configs["custom"]["omit the multiplication sign"] and \
                temp_token.type == Token.TYPE.OPERATOR and temp_token.value == '*' and \
                Lexer.omit_multiplication_sign_condition(tokens[index - 1], tokens[index + 1]):
            continue

        # å½“ä¸å¯ç”¨ä¿ç•™æ— ç†æ•°å‚æ•°æ—¶,èˆå¼ƒæ— ç†æ•°å‚æ•°
        if temp_token.type == Token.TYPE.IRRATIONAL_PARAM and not configs["custom"]["retain irrational param"]:
            continue

        elif temp_token.type == Token.TYPE.NATIVE_IRRATIONAL and configs["custom"][
            "non-ascii character form native irrational"]:
            result += ascii_native_irrational_map[temp_token.value]

        elif temp_token.type == Token.TYPE.INTEGER:
            if add_superscript:
                for char in temp_token.value:
                    result += superscript_map[char]
                    continue
            else:
                result += _add_separator(temp_token, number_seperator,
                                     configs["readability"]["number separators add thresholds"],
                                     configs["readability"]["number separator interval"])
        else:
            result += temp_token.value

        if index != len(tokens) - 1 and \
                not (configs["readability"]["superscript"] and tokens[index + 1].value == '^') and\
                not (configs["custom"]["omit the multiplication sign"] and tokens[index + 1].value == '*' and index + 2 < len(tokens) and Lexer.omit_multiplication_sign_condition(tokens[index], tokens[index + 2])):
            result += between_token

    return result


class OlocResult:
    r"""
    è¡¨è¾¾olocè®¡ç®—ç»“æœçš„ç±»ï¼Œå…·æœ‰ä¸å¯å˜æ€§ã€‚
    ä¸€æ—¦å®ä¾‹åŒ–,OlocResult çš„å±æ€§æ— æ³•ä¿®æ”¹æˆ–åˆ é™¤ã€‚

    :param expression: è¦è®¡ç®—çš„åŸå§‹è¡¨è¾¾å¼
    :param preprocessor: æ„é€ ç»“æœçš„é¢„å¤„ç†å™¨
    :param lexer: æ„é€ ç»“æœçš„è¯æ³•åˆ†æå™¨
    :param parser: æ„é€ ç»“æ„çš„è¯­æ³•åˆ†æå™¨
    :param evaluator: æ„é€ ç»“æœçš„æ±‚å€¼å™¨
    """

    def __init__(self, expression: str, preprocessor: Preprocessor, lexer: Lexer, parser: Parser, evaluator: Evaluator):

        start = time.time_ns()

        self._expression = expression
        self._preprocessor = preprocessor
        self._lexer = lexer
        self._parser = parser
        self._evaluator = evaluator

        self._result: list[str] = []

        for tokens in self._evaluator.result:
            self._result.append(output_filter(tokens))

        self._version = utils.get_version()

        self._result_time_cost = time.time_ns() - start
        self._time_cost = self._preprocessor.time_cost + self._lexer.time_cost + self._parser.time_cost + self._evaluator.time_cost + self._result_time_cost

        self._detail: dict[any] = {
            "expression": {
                "input": self._expression,
                "preprocessor": self._preprocessor.expression,
                "lexer": self._lexer.expression,
                "parser": self._parser.expression,
                "evaluator": self._evaluator.expression,
            },
            "token flow": {
                "lexer": self._lexer.tokens,
                "parser": self._parser.tokens,
                "evaluator": self._evaluator.tokens,
            },
            "ast": {
                "parser": self._parser.ast,
                "evaluator": self._evaluator.ast,
            },
            "time cost": {
                "preprocessor": self._preprocessor.time_cost,
                "lexer": self._lexer.time_cost,
                "parser": self._parser.time_cost,
                "evaluator": self._evaluator.time_cost,
                "result": self._result_time_cost
            },
            "result": self._result,
            "version": self._version,
        }

    def format_detail(self, simp: bool = True) -> str:
        r"""
        è·å–æ ¼å¼åŒ–è®¡ç®—ç»†èŠ‚
        :param simp: æ˜¯å¦è¿”å›ç®€åŒ–æ¨¡å¼ç»“æœ
        :return: æ ¼å¼åŒ–è®¡ç®—ç»†èŠ‚å­—ç¬¦ä¸²
        """

        # å®šä¹‰å­å‡½æ•°
        def _format_simple() -> str:
            r"""
            ç”Ÿæˆç®€åŒ–ç‰ˆçš„è®¡ç®—ç»†èŠ‚
            :return: ç®€åŒ–æ¨¡å¼çš„è®¡ç®—ç»†èŠ‚å­—ç¬¦ä¸²
            """
            result = f"{self._expression}\n={self._parser.expression}\n"
            for temp_result in self._result:
                result += f"={temp_result}\n"
            result += f"In {self._time_cost / 1000000:.6f} ms"
            return result

        def _format_summary() -> str:
            r"""
            ç”Ÿæˆæ‘˜è¦éƒ¨åˆ†
            :return: æ ¼å¼åŒ–åçš„æ‘˜è¦å­—ç¬¦ä¸²
            """
            result = "== Summary ==\n"
            result += f"Input Expression: {self._detail['expression']['input']}\n"
            result += f"Final Result    : {self._detail['expression']['evaluator']}\n"
            result += f"Total Time      : {self._time_cost / 1000000:.6f} ms\n"
            result += f"Steps Token     : {len(self._result)}\n\n"
            return result

        def _format_expression_flow() -> str:
            r"""
            ç”Ÿæˆè¡¨è¾¾å¼å˜åŒ–è¿‡ç¨‹
            :return: æ ¼å¼åŒ–åçš„è¡¨è¾¾å¼æµç¨‹å­—ç¬¦ä¸²
            """
            result = "== Expression Flow ==\n"
            result += f"Input       : {self._detail['expression']['input']}\n"
            result += f"Preprocessor: {self._detail['expression']['preprocessor']}\n"
            result += f"Lexer       : {self._detail['expression']['lexer']}\n"
            result += f"Parser      : {self._detail['expression']['parser']}\n"
            result += f"Evaluator   : {self._detail['expression']['evaluator']}\n\n"
            return result

        def _format_token_flow() -> str:
            r"""
            ç”ŸæˆTokenæµä¿¡æ¯
            :return: æ ¼å¼åŒ–åçš„Tokenæµå­—ç¬¦ä¸²
            """
            result = "== Token Flow ==\n"
            result += "Lexer Tokens:\n"
            for i, token in enumerate(self._detail['token flow']['lexer']):
                result += f"  [{i}] {token}\n"

            result += "\nParser Tokens:\n"
            for i, token in enumerate(self._detail['token flow']['parser']):
                result += f"  [{i}] {token}\n"

            result += "\nEvaluator Tokens:\n"
            for i, token in enumerate(self._detail['token flow']['evaluator']):
                result += f"  [{i}] {token}\n"
            return result

        def _format_ast() -> str:
            r"""
            ç”ŸæˆASTä¿¡æ¯
            :return: æ ¼å¼åŒ–åçš„ASTå­—ç¬¦ä¸²
            """
            result = "\n== Abstract Syntax Tree ==\n"
            result += f"{self._detail['ast']['parser']}\n\n"
            return result

        def _format_evaluation_process() -> str:
            r"""
            ç”Ÿæˆè®¡ç®—è¿‡ç¨‹
            :return: æ ¼å¼åŒ–åçš„è®¡ç®—è¿‡ç¨‹å­—ç¬¦ä¸²
            """
            result = "== Evaluation Process ==\n"
            for i, step in enumerate(self._result):
                if i == 0:
                    result += f"Initial: {step}\n"
                else:
                    result += f"Step {i}: {step}\n"
            result += f"Final: {self._detail['expression']['evaluator']}\n\n"
            return result

        def _format_complexity_analysis() -> str:
            r"""
            ç”Ÿæˆè¡¨è¾¾å¼å¤æ‚åº¦åˆ†æ
            :return: æ ¼å¼åŒ–åçš„å¤æ‚åº¦åˆ†æå­—ç¬¦ä¸²
            """
            tokens = self._detail['token flow']['lexer']

            # å®šä¹‰å„ç±»å‹çš„æƒé‡
            weights = {
                # åŸºæœ¬æ•°å­—ç±»å‹
                Token.TYPE.INTEGER: 1,
                Token.TYPE.FINITE_DECIMAL: 1.5,
                Token.TYPE.PERCENTAGE: 2,
                Token.TYPE.INFINITE_DECIMAL: 2.5,

                # æ— ç†æ•°ç±»å‹
                Token.TYPE.NATIVE_IRRATIONAL: 2,
                Token.TYPE.SHORT_CUSTOM: 2.5,
                Token.TYPE.LONG_CUSTOM: 3,
                Token.TYPE.IRRATIONAL_PARAM: 0,

                # è¿ç®—ç¬¦
                Token.TYPE.OPERATOR: 1.5,
                Token.TYPE.LBRACKET: 1,
                Token.TYPE.RBRACKET: 1,

                # å‡½æ•°å’Œåˆ†éš”ç¬¦
                Token.TYPE.FUNCTION: 5,
                Token.TYPE.PARAM_SEPARATOR: 0.5,

                # å…¶ä»–
                Token.TYPE.UNKNOWN: 10
            }

            # æŒ‰ç±»å‹ç»Ÿè®¡tokenæ•°é‡
            type_counts = {}
            for token in tokens:
                token_type = token.type
                if token_type not in type_counts:
                    type_counts[token_type] = 0
                type_counts[token_type] += 1

            # è®¡ç®—æ€»å¤æ‚åº¦åˆ†æ•°
            complexity_score = 0
            for token_type, count in type_counts.items():
                if token_type in weights:
                    complexity_score += weights[token_type] * count

            # åµŒå¥—æ·±åº¦åˆ†æ
            bracket_stack = []
            max_depth = 0
            current_depth = 0

            for token in tokens:
                if token.type == Token.TYPE.LBRACKET:
                    bracket_stack.append(token.value)
                    current_depth += 1
                    max_depth = max(max_depth, current_depth)
                elif token.type == Token.TYPE.RBRACKET:
                    if bracket_stack:  # æ£€æŸ¥æ ˆæ˜¯å¦ä¸ºç©º
                        bracket_stack.pop()
                        current_depth -= 1

            # å‡½æ•°åµŒå¥—åˆ†æ
            function_count = 0
            function_depth = 0
            max_function_depth = 0

            for i, token in enumerate(tokens):
                if token.type == Token.TYPE.FUNCTION:
                    function_count += 1
                    # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ‹¬å·ï¼Œåˆ†ææ˜¯å¦æ˜¯å‡½æ•°åµŒå¥—
                    if i + 1 < len(tokens) and tokens[i + 1].type == Token.TYPE.LBRACKET:
                        function_depth += 1
                        max_function_depth = max(max_function_depth, function_depth)

                        # æŸ¥æ‰¾å¯¹åº”çš„å³æ‹¬å·
                        bracket_count = 1
                        for j in range(i + 2, len(tokens)):
                            if tokens[j].type == Token.TYPE.LBRACKET:
                                bracket_count += 1
                            elif tokens[j].type == Token.TYPE.RBRACKET:
                                bracket_count -= 1
                                if bracket_count == 0:
                                    if j + 1 < len(tokens) and tokens[j + 1].type != Token.TYPE.FUNCTION:
                                        function_depth -= 1
                                    break

            # ä¸ºåµŒå¥—æ·±åº¦æ·»åŠ é¢å¤–åˆ†æ•°
            complexity_score += max_depth * 2
            complexity_score += max_function_depth * 3

            # æ ¹æ®æ€»åˆ†ç¡®å®šå¤æ‚åº¦çº§åˆ«
            complexity_level = ""
            if complexity_score <= 10:
                complexity_level = "Very Simple"
            elif complexity_score <= 20:
                complexity_level = "Simple"
            elif complexity_score <= 35:
                complexity_level = "Moderate"
            elif complexity_score <= 50:
                complexity_level = "Complex"
            elif complexity_score <= 70:
                complexity_level = "Very Complex"
            else:
                complexity_level = "Extremely Complex"

            # æ ¼å¼åŒ–è¾“å‡º
            result = "== Complexity Analysis ==\n"
            result += f"Overall Complexity: {complexity_level} (Score: {complexity_score:.1f})\n\n"

            # ç»†èŠ‚åˆ†æ
            result += "Token Distribution:\n"
            for token_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
                if token_type in weights:
                    token_weight = weights[token_type]
                    token_contribution = token_weight * count
                    token_percentage = (token_contribution / complexity_score) * 100 if complexity_score > 0 else 0
                    result += f"  {token_type}: {count} tokens Ã— {token_weight:.1f} weight = {token_contribution:.1f} ({token_percentage:.1f}%)\n"

            result += f"\nNesting Analysis:\n"
            result += f"  Maximum Bracket Depth: {max_depth}\n"
            result += f"  Function Count: {function_count}\n"
            result += f"  Maximum Function Nesting: {max_function_depth}\n\n"

            # æä¾›å¤æ‚åº¦å‡å°‘å»ºè®®
            suggestions = []
            if complexity_score > 20:
                if max_depth > 3:
                    suggestions.append("  â€¢ Consider breaking deeply nested expressions into smaller parts")

                if max_function_depth > 2:
                    suggestions.append("  â€¢ Reduce function nesting by storing intermediate function results")

                if Token.TYPE.INFINITE_DECIMAL in type_counts and type_counts[Token.TYPE.INFINITE_DECIMAL] > 1:
                    suggestions.append("  â€¢ Consider simplifying or approximating infinite decimals")

                if Token.TYPE.LONG_CUSTOM in type_counts:
                    suggestions.append("  â€¢ Use shorter variable names for custom irrational numbers")

                if len(tokens) > 30:
                    suggestions.append("  â€¢ Split long expressions into multiple calculations")

            # ä»…åœ¨æœ‰å»ºè®®æ—¶æ˜¾ç¤ºæ ‡é¢˜å’Œå»ºè®®
            if suggestions:
                result += "Complexity Reduction Suggestions:\n"
                result += "\n".join(suggestions) + "\n"

            return result

        def _format_time_cost() -> str:
            r"""
            ç”Ÿæˆæ—¶é—´æ¶ˆè€—åˆ†æ
            :return: æ ¼å¼åŒ–åçš„æ—¶é—´æ¶ˆè€—å­—ç¬¦ä¸²
            """
            # æå–å„é˜¶æ®µæ—¶é—´
            preproc_time = self._detail['time cost']['preprocessor'] / 1000000
            lexer_time = self._detail['time cost']['lexer'] / 1000000
            parser_time = self._detail['time cost']['parser'] / 1000000
            evaluator_time = self._detail['time cost']['evaluator'] / 1000000
            result_time = self._detail['time cost']['result'] / 1000000
            total_time = self._time_cost / 1000000

            result = "\n== Time Cost (ms) ==\n"
            result += f"Preprocessor: {preproc_time:.6f} ms\n"
            result += f"Lexer       : {lexer_time:.6f} ms\n"
            result += f"Parser      : {parser_time:.6f} ms\n"
            result += f"Evaluator   : {evaluator_time:.6f} ms\n"
            result += f"Result      : {result_time:.6f} ms\n"
            result += f"Total       : {total_time:.6f} ms\n\n"

            # æ·»åŠ æ¨ªæ¡å¯è§†åŒ–
            if total_time > 0:
                # è®¡ç®—ç™¾åˆ†æ¯”
                preproc_pct = (preproc_time / total_time) * 100
                lexer_pct = (lexer_time / total_time) * 100
                parser_pct = (parser_time / total_time) * 100
                evaluator_pct = (evaluator_time / total_time) * 100
                result_pct = (result_time / total_time) * 100

                # åˆ›å»ºå¯è§†åŒ–æ¨ªæ¡
                result += _format_time_bar(
                    preproc_pct, lexer_pct, parser_pct, evaluator_pct, result_pct
                )

            return result

        def _format_time_bar(preproc_pct, lexer_pct, parser_pct, evaluator_pct, result_pct) -> str:
            r"""
            ç”Ÿæˆæ—¶é—´åˆ†å¸ƒæ¨ªæ¡å¯è§†åŒ–
            :param preproc_pct: é¢„å¤„ç†å™¨æ‰€å ç™¾åˆ†æ¯”
            :param lexer_pct: è¯æ³•åˆ†æå™¨æ‰€å ç™¾åˆ†æ¯”
            :param parser_pct: è¯­æ³•åˆ†æå™¨æ‰€å ç™¾åˆ†æ¯”
            :param evaluator_pct: æ±‚å€¼å™¨æ‰€å ç™¾åˆ†æ¯”
            :param result_pct: ç»“æœå¤„ç†æ‰€å ç™¾åˆ†æ¯”
            :return: æ ¼å¼åŒ–åçš„æ—¶é—´åˆ†å¸ƒæ¨ªæ¡å­—ç¬¦ä¸²
            """
            bar_width = 60  # æ¨ªæ¡æ€»å®½åº¦

            # è®¡ç®—æ¯ä¸ªéƒ¨åˆ†çš„å­—ç¬¦æ•°
            preproc_width = max(1, int(preproc_pct * bar_width / 100)) if preproc_pct > 0 else 0
            lexer_width = max(1, int(lexer_pct * bar_width / 100)) if lexer_pct > 0 else 0
            parser_width = max(1, int(parser_pct * bar_width / 100)) if parser_pct > 0 else 0
            evaluator_width = max(1, int(evaluator_pct * bar_width / 100)) if evaluator_pct > 0 else 0
            result_width = max(1, int(result_pct * bar_width / 100)) if result_pct > 0 else 0

            # è°ƒæ•´æ€»å®½åº¦
            total_width = preproc_width + lexer_width + parser_width + evaluator_width + result_width

            # å¤„ç†èˆå…¥è¯¯å·®
            if total_width < bar_width:
                # å°†å·®å€¼æ·»åŠ åˆ°æœ€å¤§éƒ¨åˆ†
                diff = bar_width - total_width
                max_pct = max(preproc_pct, lexer_pct, parser_pct, evaluator_pct, result_pct)

                if max_pct == preproc_pct and preproc_width > 0:
                    preproc_width += diff
                elif max_pct == lexer_pct and lexer_width > 0:
                    lexer_width += diff
                elif max_pct == parser_pct and parser_width > 0:
                    parser_width += diff
                elif max_pct == evaluator_pct and evaluator_width > 0:
                    evaluator_width += diff
                elif result_width > 0:
                    result_width += diff
            elif total_width > bar_width:
                # ä»æœ€å¤§éƒ¨åˆ†å‡å»å·®å€¼
                diff = total_width - bar_width
                max_width = max(preproc_width, lexer_width, parser_width, evaluator_width, result_width)

                if max_width == preproc_width and preproc_width > diff:
                    preproc_width -= diff
                elif max_width == lexer_width and lexer_width > diff:
                    lexer_width -= diff
                elif max_width == parser_width and parser_width > diff:
                    parser_width -= diff
                elif max_width == evaluator_width and evaluator_width > diff:
                    evaluator_width -= diff
                elif result_width > diff:
                    result_width -= diff

            # ç”Ÿæˆæ¨ªæ¡
            result = "Time Distribution:\n"

            # ä¸Šè¾¹ç•Œ
            result += "+" + "-" * bar_width + "+\n"

            # æ¨ªæ¡å†…å®¹
            bar = "|"
            if preproc_width > 0:
                bar += "P" * preproc_width
            if lexer_width > 0:
                bar += "L" * lexer_width
            if parser_width > 0:
                bar += "A" * parser_width
            if evaluator_width > 0:
                bar += "E" * evaluator_width
            if result_width > 0:
                bar += "R" * result_width
            bar += "|\n"

            result += bar

            # ä¸‹è¾¹ç•Œ
            result += "+" + "-" * bar_width + "+\n"

            # å›¾ä¾‹
            result += "Legend: P=Preprocessor, L=Lexer, A=Parser, E=Evaluator, R=Result\n"

            # å„éƒ¨åˆ†ç™¾åˆ†æ¯”
            result += f"Preprocessor: {preproc_pct:.1f}%, "
            result += f"Lexer: {lexer_pct:.1f}%, "
            result += f"Parser: {parser_pct:.1f}%, "
            result += f"Evaluator: {evaluator_pct:.1f}%, "
            result += f"Result: {result_pct:.1f}%\n"

            return result

        # æ ¹æ® simp å‚æ•°é€‰æ‹©è¿”å›ç®€åŒ–æˆ–è¯¦ç»†ç‰ˆæœ¬
        if simp:
            return _format_simple()
        else:
            # åˆ›å»ºè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹å±•ç¤º
            result = "=== Oloc Calculation Detailed Report ===\n"
            result += f"Version: {self._version}\n\n"

            # æ·»åŠ å„ä¸ªè¯¦ç»†éƒ¨åˆ†
            result += _format_summary()
            result += _format_expression_flow()
            result += _format_token_flow()
            result += _format_ast()
            result += _format_evaluation_process()
            result += _format_complexity_analysis()
            result += _format_time_cost()

            return result

    @property
    def expression(self) -> str:
        r"""
        è·å–è¡¨è¾¾å¼å­—ç¬¦ä¸²ã€‚
        :return: è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        return self._expression

    @property
    def result(self) -> List[str]:
        r"""
        è·å–è¡¨è¾¾å¼è®¡ç®—ç»“æœçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        :return: ç»“æœå­—ç¬¦ä¸²åˆ—è¡¨
        """
        return self._result

    @property
    def time_cost(self) -> float:
        r"""
        è·å–æ€»è®¡ç®—è€—æ—¶
        :return: è®¡ç®—è€—æ—¶(ms)
        """
        return self._time_cost

    @property
    def detail(self) -> dict:
        r"""
        è·å–è®¡ç®—ç»†èŠ‚
        :return: è®¡ç®—ç»†èŠ‚å­—å…¸
        """
        return self._detail

    def __str__(self) -> str:
        r"""
        å°† OlocResult è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œè¿”å› result åˆ—è¡¨çš„æœ€åä¸€é¡¹ã€‚
        :return: result åˆ—è¡¨çš„æœ€åä¸€é¡¹ã€‚å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        """
        return self._result[-1] if self._result else ""

    def __repr__(self) -> str:
        r"""
        è¿”å› OlocResult å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ã€‚
        :return: å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
        """
        return f"OlocResult({self._expression} => {self._result[-1]}; {self.time_cost / 1_000_000} ms)"

    def __float__(self) -> float:
        r"""
        è½¬æ¢ä¸ºæµ®ç‚¹å‹ã€‚
        :raises OlocConversionError: å¦‚æœæ— æ³•è¿›è¡Œè½¬æ¢(å¦‚ç¼ºå¤±æ— ç†æ•°å‚æ•°çš„æ— ç†æ•°)
        :return: è½¬åŒ–åçš„æµ®ç‚¹æ•°
        """

    def __int__(self) -> int:
        r"""
        è½¬æ¢ä¸ºæ•´å‹ã€‚(å…ˆè½¬åŒ–ä¸ºæµ®ç‚¹)
        :return: è½¬åŒ–åçš„æ•´æ•°
        """
        return int(self.__float__())

    def get_fraction(self) -> Fraction:
        r"""
        è½¬åŒ–ä¸ºPythonåŸç”Ÿçš„Fractionç±»å‹ã€‚(å…ˆè½¬åŒ–ä¸ºæµ®ç‚¹)
        :return: Fractionç±»å‹çš„ç»“æœ
        """

    def __setattr__(self, name: str, value: Any) -> None:
        r"""
        ç¦æ­¢ä¿®æ”¹ OlocResult çš„å±æ€§ã€‚
        :raises AttributeError: å¦‚æœå°è¯•ä¿®æ”¹å·²å­˜åœ¨çš„å±æ€§
        """
        if hasattr(self, name):
            raise AttributeError(f"OlocResult is immutable. Cannot modify attribute '{name}'.")
        super().__setattr__(name, value)

    def __delattr__(self, name: str) -> None:
        r"""
        ç¦æ­¢åˆ é™¤ OlocResult çš„å±æ€§ã€‚
        :raises AttributeError: å¦‚æœå°è¯•åˆ é™¤å±æ€§
        """
        raise AttributeError(f"OlocResult is immutable. Cannot delete attribute '{name}'.")


"""test"""
if __name__ == "__main__":
    ...
